# -*- coding: utf-8 -*-
"""
The :mod:`parsimony.algorithms` module includes several algorithms used
throughout the package.

Algorithms may not be stateful. I.e., if they are classes, do not keep
references to objects with state in the algorithm objects. It should be
possible to copy and share algorithms between models, and thus they should not
depend on any state.

Created on Fri Feb  8 17:24:11 2013

@author:  Tommy LÃ¶fstedt
@email:   tommy.loefstedt@cea.fr
@license: TBD.
"""

import numpy as np

import parsimony.utils.math
import parsimony.utils as utils

from time import time, clock

#time_func = time
time_func = clock

__all__ = ['FastSVD', 'FastSparseSVD',
           'fista', 'conesta', 'conesta_static', 'conesta_dynamic',
           'ExcessiveGapMethod']


def FastSVD(X, max_iter=100, start_vector=None):
    """A kernel SVD implementation.

    Performs SVD of given matrix. This is always faster than np.linalg.svd.
    Particularly, this is a lot faster than np.linalg.svd when M << N or
    M >> N, for an M-by-N matrix.

    Arguments:
    ---------
    X : The matrix to decompose

    Returns:
    -------
    v : The right singular vector.
    """
    M, N = X.shape
    if M < 80 and N < 80:  # Very arbitrary threshold for my computer ;-)
        _, _, V = np.linalg.svd(X, full_matrices=True)
        v = V[[0], :].T
    elif M < N:
        K = np.dot(X, X.T)
        # TODO: Use module for this!
        t = np.random.rand(X.shape[0], 1)
#        t = start_vectors.RandomStartVector().get_vector(Xt)
        for it in xrange(max_iter):
            t_ = t
            t = np.dot(K, t_)
            t /= np.sqrt(np.sum(t ** 2.0))

            if np.sqrt(np.sum((t_ - t) ** 2.0)) < utils.TOLERANCE:
                break

        v = np.dot(X.T, t)
        v /= np.sqrt(np.sum(v ** 2.0))

    else:
        K = np.dot(X.T, X)
        # TODO: Use module for this!
        v = np.random.rand(X.shape[1], 1)
        v /= utils.math.norm(v)
#        v = start_vectors.RandomStartVector().get_vector(X)
        for it in xrange(max_iter):
            v_ = v
            v = np.dot(K, v_)
            v /= np.sqrt(np.sum(v ** 2.0))

            if np.sqrt(np.sum((v_ - v) ** 2.0)) < utils.TOLERANCE:
                break

    return v


def FastSparseSVD(X, max_iter=100, start_vector=None):
    """A kernel SVD implementation for sparse CSR matrices.

    This is usually faster than np.linalg.svd when density < 20% and when
    M << N or N << M (at least one order of magnitude). When M = N >= 10000 it
    is faster when the density < 1% and always faster regardless of density
    when M = N < 10000.

    These are ballpark estimates that may differ on your computer.

    Arguments:
    ---------
    X : The matrix to decompose

    Returns:
    -------
    v : The right singular vector.
    """
    M, N = X.shape
    if M < N:
        K = X.dot(X.T)
#        t = X.dot(p)
        # TODO: Use module for this!
        t = np.random.rand(X.shape[0], 1)
        for it in xrange(max_iter):
            t_ = t
            t = K.dot(t_)
            t /= np.sqrt(np.sum(t ** 2.0))

            a = float(np.sqrt(np.sum((t_ - t) ** 2.0)))
            if a < utils.TOLERANCE:
                break

        v = X.T.dot(t)
        v /= np.sqrt(np.sum(v ** 2.0))

    else:
        K = X.T.dot(X)
        # TODO: Use module for this!
        v = np.random.rand(X.shape[1], 1)
        v /= utils.math.norm(v)
#        v = start_vectors.RandomStartVector().get_vector(X)
        for it in xrange(max_iter):
            v_ = v
            v = K.dot(v_)
            v /= np.sqrt(np.sum(v ** 2.0))

            a = float(np.sqrt(np.sum((v_ - v) ** 2.0)))
            if a < utils.TOLERANCE:
                break

    return v


def fista(X, y, function, beta, step=None, mu=None,
          eps=utils.TOLERANCE,
          max_iter=utils.MAX_ITER, min_iter=1, b_star=None, gradL1=None):
    """ The fast iterative shrinkage threshold algorithm.
    """
    z = betanew = betaold = beta

    if mu == None:
        mu = 0.9 * function.mu(beta)

    if step == None:
        step = 1.0 / function.Lipschitz(X, mu, max_iter=max_iter)

    t = []
    f = []
    b = []
    g = []
    for i in xrange(1, max_iter + 1):
        tm = time_func()

        z = betanew + ((i - 2.0) / (i + 1.0)) * (betanew - betaold)
        betaold = betanew
        betanew = function.prox(z - step * function.grad(X, y, z, mu), step)

        t.append(time_func() - tm)
        f.append(function.f(X, y, betanew, mu=0.0))
        if b_star != None:
            b.append(utils.math.norm(betanew - b_star))
        if gradL1 != None:
            g.append(utils.math.norm(function.grad(X, y, betanew, mu) \
                                      + gradL1))
        else:
            g.append(utils.math.norm(function.grad(X, y, betanew, mu)))

        if (1.0 / step) * utils.math.norm(betanew - z) < eps and i >= min_iter:
            break

    # TODO: These return values are for the OLS paper. Will be changed!
    return (betanew, f, t, b, g)


def conesta(X, y, function, beta, mu_start=None, mumin=utils.TOLERANCE,
            tau=0.5, dynamic=True,
            eps=utils.TOLERANCE,
            conts=50, max_iter=1000, min_iter=1, b_star=None, gradL1=None):

    if mu_start != None:
        mu = [mu_start]
    else:
        mu = [0.9 * function.mu(beta)]
    print "mu0:", mu[0]

    tmin = 1.0 / function.Lipschitz(X, mumin, max_iter=1000)

    max_eps = function.eps_max(mu[0])

#    G = eps0 = min(max_eps, function.eps_opt(mu[0], X))
    G = min(max_eps, function.eps_opt(mu[0], X))

    beta_hat = None

    t = []
    f = []
    b = []
    g = []
    Gval = []

    i = 0
    while True:
        stop = False

        tnew = 1.0 / function.Lipschitz(X, mu[-1], max_iter=100)
        eps_plus = min(max_eps, function.eps_opt(mu[-1], X))
        (beta, fval, tval, bval, gval) = fista(X, y, function, beta, tnew,
                                               mu[-1], eps=eps_plus,
                                               max_iter=max_iter,
                                               min_iter=1,
                                               b_star=b_star, gradL1=gradL1)
        print "fista did iterations =", len(fval)

        mumin = min(mumin, mu[-1])
        tmin = min(tmin, tnew)
        beta_tilde = function.prox(beta - tmin * function.grad(X, y,
                                                               beta, mumin),
                                   tmin)

        if (1.0 / tmin) * utils.math.norm(beta - beta_tilde) < eps or i >= conts:
            print "%f < %f" % ((1. / tmin) * utils.math.norm(beta - beta_tilde), eps)
            print "%d >= %d" % (i, conts)
            stop = True

        gap_time = time_func()
        if dynamic:
            G_new, beta_hat = function.gap(X, y, beta, beta_hat,
                                           mu[-1], eps=eps_plus)
            G_new = abs(G_new)  # Just in case ...
        else:
#            G_new = eps0 * tau ** (i + 1)
            G_new = tau * G
#            print "Diff:", abs(G_new - G_new_)

        if G_new < G:  # Always happens in the static version
            G = G_new
        else:
            G = tau * G
#        G = tau * min(G, abs(G_new))

        gap_time = time_func() - gap_time
        print "Gap:", G
        Gval.append(G)

        f = f + fval
        tval[-1] += gap_time
        t = t + tval
        b = b + bval
        g = g + gval

        # For the simulation we make sure that we do enough iterations
        if len(f) < 0.33 * conts * max_iter or len(fval) < 0.9 * max_iter:
            stop = False

        if (G <= utils.TOLERANCE and mu[-1] <= utils.TOLERANCE) or stop:
            break

        mu_new = min(mu[-1], function.mu_opt(G, X))
        mumin = min(mumin, mu_new)  # Testing only. Remove!!
        mu = mu + [max(mumin, mu_new)] * len(fval)

        i = i + 1

    # TODO: These return values are for the OLS paper. Will be changed!
    return (beta, f, t, mu, Gval, b, g)

def conesta_static(*args, **kwargs):
    return conesta(dynamic=False, *args, **kwargs)

def conesta_dynamic(*args, **kwargs):
    return conesta(dynamic=True, *args, **kwargs)


def ExcessiveGapMethod(X, y, function, eps=utils.TOLERANCE,
                       max_iter=utils.MAX_ITER, b_star=None, f_star=0.0):
    """ The excessive gap method for strongly convex functions.

    Parameters
    ----------
    function : The function to minimise. It contains two parts, function.g is
            the strongly convex part and function.h is the smoothed part of the
            function.
    """
    A = function.h.A()

    u = [0] * len(A)
    for i in xrange(len(A)):
        u[i] = np.zeros((A[i].shape[0], 1))

    # L = lambda_max(A'A) / (lambda_min(X'X) + k)
    L = function.Lipschitz(X, max_iter=1000000)
    print "L:", L

    mu = [2.0 * L]
    beta0 = function.betahat(X, y, u)  # u is zero here
    beta = beta0
    alpha = function.V(u, beta, L)  # u is zero here

#    print "f  :", function.g.f(X, y, beta) + function.h.f(beta, mu[0])
#    print "phi:", function.g.f(X, y, beta) + function.h.phi(beta, alpha, mu=0.0)

    t = []
    f = []
    b = []
    ulim = []

    k = 0

#    _f = []
#    _phi = []

    while True:
        tm = time_func()

        tau = 2.0 / (float(k) + 3.0)

        alpha_hat = function.h.alpha(beta, mu[k])
        for i in xrange(len(alpha_hat)):
            u[i] = (1.0 - tau) * alpha[i] + tau * alpha_hat[i]

        mu.append((1.0 - tau) * mu[k])
        betahat = function.betahat(X, y, u)
        beta = (1.0 - tau) * beta + tau * betahat
        alpha = function.V(u, betahat, L)

#        _f.append(function.g.f(X, y, beta) + function.h.f(beta, mu[k+1]))
#        _phi.append(function.g.f(X, y, betahat) + function.h.phi(betahat, alpha, mu=0.0))

        t.append(time_func() - tm)
        f.append(function.f(X, y, beta, mu=0.0))
        if b_star != None:
            b.append(utils.math.norm(beta - b_star))
#        ulim.append(2.0 * function.h.M() * mu[0] / ((float(k) + 1.0) * (float(k) + 2.0)))
        ulim.append(mu[k + 1] * function.h.M())

        if ulim[-1] < eps or k >= max_iter:
            break

        k = k + 1

    print "L:", L
    print "mu[-1]:", mu[-1]

#    import matplotlib.pyplot as plot
#    plot.plot([_f[i] - f_star for i in xrange(len(_f))], 'r')
#    plot.plot([_phi[i] - f_star for i in xrange(len(_phi))], 'g')
#    plot.plot([f[i] - f_star for i in xrange(len(f))])
#    plot.plot(ulim, '-.r')
#    plot.show()

    # TODO: These return values are for the OLS paper. Will be changed!
    return (beta, f, t, mu, ulim, beta0, b)